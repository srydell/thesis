\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage[autoplay]{animate}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

%\usepackage{pgfplots}
%\usepgfplotslibrary{dateplot}

\usepackage{xspace}

\setbeamercolor{normal text}{bg=white}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Hausdorff dimension of Clusters Generated by the Worm algorithm}
%\subtitle{A modern beamer theme}
\date{\today}
\date{}
\author{Simon Rydell}
\institute{Royal Institute of Technology, Stockholm}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Fractals}
% The term fractal dimension was coined by Mandelbrot in 1975 where the dimension of some object is not limited to that of whole numbers, but can be any real number. When many of us think of fractals we may think of self-similar fractals such as the Koch curve or the Sierpinski triangle, but this is overly restrictive as we shall see.



% Let's start with the example of a square and the Sierpinski triangle. Each square can be divided into four identical squares, only scaled down by a factor of one half. The Sierpinski triangle can be divided into thirds, each with the length of one half the original triangle.
% Now think of the square as being made of sheet metal, and the triangle as a metal mesh. Then the mass of the square is scaled down by one fourth, or one half to the power of two. So one way to define the dimension is as the answer to the question, if I scale this object by some factor s, then the mass of that object is scaled by s to the power d, which we call the dimension.
% Applying this logic to the Sierpinski triangle, we scale the linear length by one half, and since it's self similar, the mass goes down by one third, giving the dimension log_2 (3) = 1.585
% This way of scaling the object and fitting its scaled down version into itself relies is called the self similarity dimension. But what about a disk? When scaled by a factor of two, its mass scales by a factor of four, but there is no obvious way of fitting four smaller disks into a larger one. 
% So we approximate. One way of doing so is to use the box counting method.

\begin{frame}{Scaling Mass}
% First row
    \begin{columns}[c]
        \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/square.pdf}
        \end{figure}
    \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/sierpinski2.pdf}
        \end{figure}
    \end{columns}
% Second row
    \begin{columns}[c]
        \column{1.5in}
            \begin{align*}
%                L &\rightarrow \frac{1}{2} L \\
%                \left (\frac{1}{2} \right )^d M &= \frac{1}{4} M \\
%                d &= 2
            \end{align*}
        \column{1.5in}
            \begin{align*}
%                L &\rightarrow \frac{1}{2} L \\
%                \left (\frac{1}{2} \right )^d M &= \frac{1}{3} M \\
%                d &= \log_2 (3) \\ &\approx 1.585
            \end{align*}
    \end{columns}
\end{frame}

\begin{frame}{Scaling Mass}
% First row
    \begin{columns}[c]
        \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/square.pdf}
        \end{figure}
    \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/sierpinski2.pdf}
        \end{figure}
    \end{columns}
% Second row
    \begin{columns}[c]
        \column{1.5in}
            \begin{align*}
                L &\rightarrow \frac{1}{2} L \\
                \left (\frac{1}{2} \right )^d M &= \frac{1}{4} M \\
%                d &= 2
            \end{align*}
        \column{1.5in}
            \begin{align*}
                L &\rightarrow \frac{1}{2} L \\
                \left (\frac{1}{2} \right )^d M &= \frac{1}{3} M \\
%                d &= \log_2 (3) \\ &\approx 1.585
            \end{align*}
    \end{columns}
\end{frame}

\begin{frame}{Scaling Mass}
% First row
    \begin{columns}[c]
        \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/square.pdf}
        \end{figure}
    \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{figures/sierpinski2.pdf}
        \end{figure}
    \end{columns}
% Second row
    \begin{columns}[c]
        \column{1.5in}
            \begin{align*}
                L &\rightarrow \frac{1}{2} L \\
                \left (\frac{1}{2} \right )^d M &= \frac{1}{4} M \\
                d &= 2
            \end{align*}
        \column{1.5in}
            \begin{align*}
                L &\rightarrow \frac{1}{2} L \\
                \left (\frac{1}{2} \right )^d M &= \frac{1}{3} M \\
                d &= \log_2 (3) \\ &\approx 1.585
            \end{align*}
    \end{columns}
\end{frame}

% It divides the graph into smaller and smaller squares, called boxes in which we can count how many is needed to cover the object. The idea is that the number of boxes needed, will scale as one over the box length, epsilon, raised to the power of the dimension. This way, if the limit exists, the dimension is well defined. It's essentially approximating the integral of the object!
\begin{frame}{Box Counting Method}
% First row
    \begin{columns}[c]
        \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=1.3\textwidth]{figures/circle_within_boxes.pdf}
        \end{figure}
        \column{1.5in}
            \begin{align*}
                N &\sim \frac{1}{\epsilon^d} \\
%                d &= \lim_{\epsilon \to 0} \frac{\ln N(\epsilon)}{\ln 1 / \epsilon}
            \end{align*}
    \end{columns}
\end{frame}

\begin{frame}{Box Counting Method}
% First row
    \begin{columns}[c]
        \column{1.5in}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=1.3\textwidth]{figures/circle_within_boxes.pdf}
        \end{figure}
        \column{1.5in}
            \begin{align*}
                N &\sim \frac{1}{\epsilon^d} \\
                d &= \lim_{\epsilon \to 0} \frac{\ln N(\epsilon)}{\ln 1 / \epsilon}
            \end{align*}
    \end{columns}
\end{frame}



\begin{frame}{Hausdorff Dimension}
    hi
\end{frame}

\section{Algorithms for Working with Graph Patterns}

\begin{frame}{Worm Algorithm}
    \begin{enumerate}[$\bullet$]
        \item Graph configurations
        \item Metropolis Steps
    \end{enumerate}
% The idea with the worm algorithm is to simulate spin, or more generally, graph configurations in order to sample the partition function. 
% Intuitively, the ‘worm’ part can be seen as a ‘magic marker’ that can connect or remove the connection between two sites.
% This way the marker draws patterns onto a lattice that corresponds to a configuration. In this thesis we are only concerned with whenever the worm reaches the same lattice site that it started on, forming loops, or clusters.
% This is done via metropolis steps and thus ensures that it adheres to detailed balance. 
\end{frame}

\begin{frame}{Labeling and Box Dimension}
    \begin{enumerate}[$\bullet$]
        \item Hoshen Kopelman Algorithm
        \item Graph Dividing
    \end{enumerate}
% When a suitable number of loops have been formed, the largest cluster need to be identified. For this a labeling algorithm named after two phycisists called Hoshen and Kopelman was used. It takes a raster scan of the graph and labels the clusters recursively.
% If only the scaling dimension was of interest, the simulation could be sampled from here, but for the box dimension, the graph needs to be divided into smaller and smaller boxes. Here I utilized that all graphs simulated in this thesis had the linear system size of 2^n for some integer n. Therefore I could recursively divide each dimension in factors of 2. So for example, a 2D square would be divided into 4 squares, and a 3D cube into 8 smaller cubes and so on.
\end{frame}

\section{Ising Model}

\begin{frame}{Ising Loop Expansion}
    \begin{equation*}
        Z &\propto  \sum_{\{S\}} \left ( 1 + \tanh(K) \sum_{l = 1} S_i S_j + \tanh^2(K) \sum_{l = 2} (S_i S_j)(S_{i'} S_{j'}) + \ldots \right)
    \end{equation*}
% The idea is to expand the ising partition to these sums over pairs of spins. The sum subscript represent the contiguous length of this link. 
\end{frame}

\begin{frame}{Ising Loop Expansion}
\begin{figure}[h!]
    \begin{subfigure}{.4\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ising_loop_one_link.pdf}
        \caption{$(S_1 S_2), \ L = 1$}
    \end{subfigure}%
    \begin{subfigure}{.4\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ising_loop_two_link.pdf}
        \caption{$(S_1 S_2)(S_2 S_4), \ L = 2$}
    \end{subfigure}\\[1ex]
    \begin{subfigure}{.8\linewidth}
        \centering
        \includegraphics[width=.5\textwidth]{figures/ising_loop_four_link.pdf}
        \caption{$(S_1 S_2)(S_2 S_4)(S_4 S_3)(S_3 S_1), \ L = 4$}
    \end{subfigure}
\end{figure}
% Here we can see the link structure in between sites. Since the partition function includes a sum over all states of each individual spin, in this case +1 and -1, only the terms with a full loop will survive, since these only have terms such as $S_i^2$.
\end{frame}


\begin{frame}{Box Dimension}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/box_dim_128x128Ising.pdf}
    \end{figure}
% Given enough sampling, a physical system will emerge. This can then be divided as per the discussed box counting method. The result is here compared to the theoretical one given by Loewner expansion.
\end{frame}

\begin{frame}{Scaling Dimension}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/maximum_loop_length_for_2D_Ising.pdf}
    \end{figure}
% If we instead use the scaling method, the result is this. The theoretical value can almost be seen under the line fit. The red line here is just for comparison.
\end{frame}

\begin{frame}{Comparison of Dimensions $2D$ Ising}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/dimenson_comparison.pdf}
% Peculiar enough, the clusters has almost the same hausdorff dimension as a self avoiding walk, shown here in yellow. This suggest some similarities between the structure of the SAW and the ising clusters
    \end{figure}
\end{frame}

\begin{frame}{Largest Ising Loop on a $128^2$ Lattice}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/largest_cluster_testing_nolattice.pdf}
    \end{figure}
% If we further examine a cluster on a 128^3 ising lattice we can see the familiar `swollen' structure in the loop. The only visible difference here is that `twisted loops' are allowed as it doesn't cost any energy to create them in the worm algorithm.
% To further get intuition about how this process is done, I created a small animation of a 2D Ising lattice. From the point where some clusters have formed, we identify the largest cluster with the Hoshen Kopelman algorithm and the lattice is divided into smaller and smaller boxes.
\end{frame}

\begin{frame}{$2D$ Ising Animation}
    % TODO: Uncomment
    %  \animategraphics[width=\linewidth,loop]{10}{figures/2dising_animation}{}{}
\end{frame}

\section{XY Model}
% Rotors... Loop Expansion. Adds complexity as a direction and weight. Use Villain approximation $\Rightarrow$ Displaces $T_c$ $\Rightarrow$ Use winding number to `find' $T_c$.

\begin{frame}{XY Loop Expansion}
    \begin{align*}
        H &= - J \sum_{\langle ij \rangle} \cos(\theta_i - \theta_j) \\
        Z &= \prod_i \int \frac{\mathrm d \theta_i}{2 \pi} \prod_{\langle ij \rangle} e^{K  \cos(\theta_i - \theta_j)}
% The XY model allows spin to rotate around some axis in a plane. Giving a Hamiltonian as a function of the difference in angle in this plane. 
% Therefore we get the 2 pi symmetric partition function, that can be fourier transformed as
    \end{align*}
\end{frame}

\begin{frame}{XY Loop Expansion}
    \begin{align*}
        Z &\sim \int \frac{\mathrm d \theta_i}{2 \pi} e^{i \sum_{\langle ij \rangle} j_{\langle ij \rangle} (\theta_i - \theta_j)} \\
% Where lower case j here is the fourier coefficients summed over the nearest neighbours. Expanding this integral shows
    \end{align*}
\end{frame}

\begin{frame}{XY Loop Expansion}
    \begin{align*}
        Z &\sim \int \frac{\mathrm d \theta_i}{2 \pi} e^{i \sum_{\langle ij \rangle} j_{\langle ij \rangle} (\theta_i - \theta_j)} \\
        & \sim \delta_{0, \sum_{\langle ij \rangle} j_{\langle ij \rangle}}
% that the sum of the links from each site to its neighbours must be zero. We can convince ourselves that this in turn means that the links together will form loops.
% This does add a bit of complexity compared to the Ising loop expansion. Here each link can have either positive or negative values, so we interpret this as direction. So instead of connecting site a and b, we connect site a to b with some weight w, giving something like this...
    \end{align*}
\end{frame}

\begin{frame}{XY Loop expansion}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/percolatingFlux.pdf}
    \end{figure}
% To simplify the acceptance probability I used the Villain approximation where the energy is simply the sum of these fourier coefficients squared.
\end{frame}

\begin{frame}{Villain Approximation}
    \begin{equation*}
        E = \frac{1}{2} \sum_i j_i^2
    \end{equation*}
% In this approximation, the notion of temperature is exchanged for one over temperature, leading to a shift in the critical temperature. One way to find the new Tc is to examine the superfluid density of the system, and notice that it is proportional to an easily measurable quantity called the winding number
\end{frame}


\begin{frame}{Winding Number}
    \begin{equation*}
        \rho_s = L^{2 - d} T \langle W_\mu^2 \rangle 
    \end{equation*}
% Where rho_s is the superfluid density and W_mu is the winding number in the mu direction. Intuitively, this says something about how many times the current percolates in the mu direction.
% We can then plot the winding number for a range of temperatures and system sizes to look for an intersection where the superfluid density goes to zero.
\end{frame}

\begin{frame}{Winding Number}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/winding_number_Tc_zoomed.pdf}
    \end{figure}
% which is done here. The darker the blue, the larger the system is, and the intersection showed in red is taken as the weighted average of the intersections of all system sizes.
% Now that the critical temperature is known, the fractal dimension of the clusters can be examined.
\end{frame}

\begin{frame}{$3D$ XY Model Hausdorff Dimension}
    \begin{align*}
        \bullet \ &\text{Hove, Mo and Sudbo (2000):}\ &D_H = 2.287 \pm 4 \cdot 10^{-3} \\
        \bullet \ &\text{Prokof'ev and Svistunov Comment (2005):} \ &D_H = 1.765 \pm 2 \cdot 10^{-3}
    \end{align*}
% In the year 2000 Hove, Mo and Sudbo published an article with the result of 2.287 for the Hausdorff dimension of an XY system. This was refuted by Prokof'ev and Svistunov in 2005 with a comment, arguing that Hove, Mo and Sudbo made a mistake in their self-similar expression for the dimension. Their result clearly differs from Hove, Mo and Sudbo's.
% Recreating this with the box dimension...
\end{frame}

\begin{frame}{Box Counting Method $3D$ XY}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/box_dimension_xy_128x3.pdf}
    %    \caption{}
    %    \label{fig:}
    \end{figure}
% ...clearly favours Prokof'ev and Svistunov with the value from this thesis being 1.77468. Here Prokof'evs results is labeled D_H^P, while Sudbo's D_H^S. In 2006, Sudbo replied, and the issue concerned if the interactions between the loops are local or not. In this thesis the interactions are completely local, and it does not necessarily correspond to the same fractal dimension.
\end{frame}

\begin{frame}{Comparison of Dimensions $3D$ XY}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/dimenson_comparison_XY.pdf}
    \end{figure}
% Here is a final comparison without the extra data from my simulations. Now to get a bit more intuition about how these systems actually form I will show two animations, one of a smaller system where each step in the worm algorithm can be seen clearly, and one of the largest cluster of a system more closely to those I've actually measured on.
\end{frame}


\begin{frame}{$3D$ XY Animation - $4^3$ system}
%  \animategraphics[width=\linewidth,loop]{10}{figures/3dxy_animation}{}{}
% So here is a completely blank system, where no configurations have been formed. As you can see there are periodic boundary conditions and the size of each arrow represents the weight between the two sites.
\end{frame}

\begin{frame}{$3D$ XY Animation - Largest cluster}
%  \animategraphics[width=\linewidth,loop]{10}{figures/3dxy_largest_cluster.pdf}{}{}
% So here is a 64^3 system where the sites that are linked in the largest cluster is highlighted in red. So this lattice would then be divided into boxes and checked what the minimal cover is.
\end{frame}

\begin{frame}{Summary}
    \begin{table}
        \parbox{.45\linewidth}{
            \centering
            \begin{tabular}{l|l}
                               & $D_H$          \\ \hline
                Box            & $1.35193(5)$   \\ \hline
                Scaling        & $1.38(2)$      \\ \hline
                $D_H^G$        & $1.375$        \\ \hline
                SAW            & $1.33$         \\ \hline
                Random Walk    & $2$                          
            \end{tabular}
            \caption{$2D$ Ising}
        }
        \hfill
        \parbox{.45\linewidth}{
            \centering
            \begin{tabular}{l|l}
                            & $D_H$           \\ \hline
                Box         & $1.77468(4)$    \\ \hline
                Prokof'ev   & $1.765(2)$      \\ \hline
                Sudbo       & $2.287(2)$  
            \end{tabular}
            \caption{$3D$ XY}
        }
    \end{table}
% Finally, here is a small summary of the major results in this thesis. Thank you. Any questions?
\end{frame}

\appendix
% --------------- Backup slides ---------------

% --------------- Ising ---------------
\begin{frame}[fragile]{Extra slides: Box Dimension $64^2$ Ising}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/box_dim_64x64Ising.pdf}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Extra slides: Susceptibility $2D$ Ising}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/susceptibility128x128Ising.pdf}
    \end{figure}
\end{frame}

% --------------- Graph Dividing ---------------

\begin{frame}[fragile]{Extra slides: Graph Dividing Algorithm}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/graphDividing.pdf}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Extra slides: Graph Dividing Algorithm}
    \begin{figure}[h!]
        \centering
            \includegraphics[width=0.8\textwidth]{figures/bench_graph_div.pdf}
    \end{figure}
\end{frame}


\end{document}